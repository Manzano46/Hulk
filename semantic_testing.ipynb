{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probando por separado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.evaluation import evaluate_reverse_parse\n",
    "\n",
    "from engine.Lexer.hulk_lexer import HulkLexer \n",
    "from engine.language.tokens_type import hulk_tokens\n",
    "from engine.language.grammar import G\n",
    "from engine.Parser.hulk_parser import HulkParser\n",
    "\n",
    "hulk_lexer = HulkLexer(hulk_tokens, G.EOF)\n",
    "hulk_parser = HulkParser(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_cases import test_cases\n",
    "\n",
    "tokens = []\n",
    "right_parses = []\n",
    "operations = []\n",
    "asts = []\n",
    "\n",
    "for input in test_cases:\n",
    "    token = hulk_lexer(input)\n",
    "    tokens.append(token)\n",
    "\n",
    "    right_parse, operation = hulk_parser(token)\n",
    "    right_parses.append(right_parse)\n",
    "    operations.append(operation)\n",
    "\n",
    "    ast = evaluate_reverse_parse(right_parse, operation, token)\n",
    "    asts.append(ast)\n",
    "\n",
    "    print(token)\n",
    "    print(right_parse)\n",
    "    print(operation)\n",
    "    print(ast)\n",
    "    print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from engine.semantic.type_builder_visitor import TypeBuilder\n",
    "# from engine.semantic.type_checker_visitor import TypeChecker\n",
    "from engine.semantic.type_collector_visitor import TypeCollector\n",
    "from engine.semantic.type_inferer_visitor import TypeInferer\n",
    "from engine.semantic.variable_collector_visitor import VarCollector\n",
    "\n",
    "\n",
    "def semantic_analysis_pipeline(ast, debug=False):\n",
    "    # if debug:\n",
    "    #     formatter = Formatter()\n",
    "    #     formatted_ast = formatter.visit(ast)\n",
    "    #     print('===================== AST =====================')\n",
    "    #     print(formatted_ast)\n",
    "    if debug:\n",
    "        print('============== COLLECTING TYPES ===============')\n",
    "    errors = []\n",
    "    collector = TypeCollector(errors)\n",
    "    collector.visit(ast)\n",
    "    context = collector.context\n",
    "    if debug:\n",
    "        print('Errors: [')\n",
    "        for error in errors:\n",
    "            print('\\t', error)\n",
    "        print(']')\n",
    "        print('Context:')\n",
    "        print(context)\n",
    "        print('=============== BUILDING TYPES ================')\n",
    "    builder = TypeBuilder(context, errors)\n",
    "    builder.visit(ast)\n",
    "    if debug:\n",
    "        print('Errors: [')\n",
    "        for error in errors:\n",
    "            print('\\t', error)\n",
    "        print(']')\n",
    "        print('Context:')\n",
    "        print(context)\n",
    "        print('=============== CHECKING TYPES ================')\n",
    "        print('---------------- COLLECTING VARIABLES ------------------')\n",
    "    var_collector = VarCollector(context, errors)\n",
    "    scope = var_collector.visit(ast)\n",
    "    if debug:\n",
    "        print('Errors: [')\n",
    "        for error in errors:\n",
    "            print('\\t', error)\n",
    "        print(']')\n",
    "        print('Context:')\n",
    "        print(context)\n",
    "        print('Scope:')\n",
    "        print(scope)\n",
    "        print('---------------- INFERRING TYPES ------------------')\n",
    "    type_inferrer = TypeInferer(context, errors)\n",
    "    type_inferrer.visit(ast)\n",
    "\n",
    "    # Check if there are any inference errors and change the types to ErrorType if there are\n",
    "    # inference_errors = context.inference_errors() + scope.inference_errors()\n",
    "    # errors.extend(inference_errors)\n",
    "    if debug:\n",
    "        # print('Iterations: ' + str(type_inferrer.current_iteration))\n",
    "        print('Errors: [')\n",
    "        for error in errors:\n",
    "            print('\\t', error)\n",
    "        print(']')\n",
    "        print('Context:')\n",
    "        print(context)\n",
    "        print('Scope:')\n",
    "        print(scope)\n",
    "        print('---------------- CHECKING TYPES ------------------')\n",
    "    # type_checker = TypeChecker(context, errors)\n",
    "    # type_checker.visit(ast)\n",
    "    # if debug:\n",
    "    #     print('Errors: [')\n",
    "    #     for error in errors:\n",
    "    #         print('\\t', error)\n",
    "    #     print(']')\n",
    "        # print('Context:')\n",
    "        # print(context)\n",
    "        # print('Scope:')\n",
    "        # print(scope)\n",
    "    return ast, errors, context, scope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(semantic_analysis_pipeline(asts[5],debug=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(semantic_analysis_pipeline(asts[7],debug=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
